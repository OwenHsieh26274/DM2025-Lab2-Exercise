{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name: 謝諶煜\n",
    "\n",
    "Student ID: 114065504\n",
    "\n",
    "GitHub ID: owenhsieh26274\n",
    "\n",
    "Kaggle name: Owen\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "![pic_ranking.png](pics/score.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
    "\n",
    "You can delete the syntax suggestions after you use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **Project Report**\n",
    "\n",
    "**Syntax:** `#` creates the largest heading (H1).\n",
    "\n",
    "---\n",
    "**Syntax:** `---` creates a horizontal rule (a separator line).\n",
    "\n",
    "## 1. Model Development (10 pts Required)\n",
    "\n",
    "**Syntax:** `##` creates a secondary heading (H2).\n",
    "\n",
    "**Describe briefly each section, you can add graphs/charts to support your explanations.**\n",
    "\n",
    "### 1.1 Preprocessing Steps\n",
    "\n",
    "**Data Loading**\n",
    "- Load raw post data from the source file, and extract `post_id` and `text` fields into a Pandas DataFrame.\n",
    "- Load split information (`train` / `test`) and emotion labels from CSV files.\n",
    "- Map split types and emotion labels to each post using `post_id`.\n",
    "<br></br>\n",
    "\n",
    "**Text Cleaning**\n",
    "- Remove `#` symbols from post text to avoid confusing model when training.\n",
    "<br></br>\n",
    "\n",
    "**Data Validation**\n",
    "- Compute maximum word count in text (max = 89) to guide tokenizer `max_length` (128 is sufficient).\n",
    "- Check for missing values:\n",
    "- Inspect emotion label distribution.\n",
    "<br></br>\n",
    "\n",
    "**Dataset Splitting**\n",
    "- Create:\n",
    "  - `train_df`: training data with emotion labels.\n",
    "  - `test_df`: test data without emotion labels.\n",
    "- Remove the `split` column after separation and reset indices.\n",
    "<br></br>\n",
    "\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "\n",
    "**Label Encoding**\n",
    "- Define a dictionary `emotion_to_id` and its reverse mapping `id_to_emotion` to map emotion names to numeric class IDs. This is for model's training and predicting.\n",
    "- Convert emotion labels in `train_df` from strings to integer class IDs and store them in a new column `labels`.\n",
    "\n",
    "### 1.3 Explanation of Your Model\n",
    "\n",
    "**Dataset Preparation & Metrics**\n",
    "- Split labeled data into training (90%) and validation (10%) sets using stratified sampling on emotion labels.\n",
    "- Convert Pandas DataFrames into Hugging Face `Dataset` objects.\n",
    "- Tokenize text with a maximum length of 128 and truncate longer sequences.\n",
    "- Remove unused columns (`text` and `emotion` since we already have tokens) and format datasets as PyTorch tensors.\n",
    "- Define evaluation metrics:\n",
    "  - **Accuracy**\n",
    "  - **Macro F1-score** (primary metrics, as the class dristribution is imbalanced).\n",
    "\n",
    "**Model Training**\n",
    "- Use the pretrained model **`SamLowe/roberta-base-go_emotions`** from Hugging Face, as this model is pretrained with emotion related datasets.\n",
    "- Initialize tokenizer and sequence classification model\n",
    "  - 6 emotion labels\n",
    "    - Since the classification head of the pretrained model doesn't fit our output format (pretrained head is to predict 28 emotions), we use `AutoModelForSequenceClassification` to automatically initiate a new one. The old one will be discarded.\n",
    "  - Custom `id2label` and `label2id` mappings\n",
    "- Configure training with Hugging Face `Trainer`:\n",
    "  - Epoch-based evaluation and checkpointing\n",
    "  - Best model selected by highest macro F1-score\n",
    "  - Learning rate: `1e-4`, batch size: `16` (train), `32` (eval)\n",
    "  - Train for 3 epochs with weight decay\n",
    "- Train the model with trainer and save both the model and tokenizer.\n",
    "\n",
    "**Prediction & Submission**\n",
    "- Load the trained model using a Hugging Face `pipeline` for text classification.\n",
    "- Run inference on test set.\n",
    "- Convert predicted labels into a submission DataFrame.\n",
    "- Export final results to `submission.csv`.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bonus Section (5 pts Optional)\n",
    "\n",
    "**Add more detail in previous sections**\n",
    "\n",
    "### 2.1 Mention Different Things You Tried\n",
    "\n",
    "- **Different pretrained models** on Hugging Face, found `SamLowe/roberta-base-go_emotions` has the best performance.\n",
    "- **Custom classification head**, no significant gain in performance compare to automatically created one.\n",
    "- **Weighted trainer**, use weighted custom trainer to ensure an even distribution of different classes within each training batch, no significant performance gain.\n",
    "\n",
    "### 2.2 Mention Insights You Gained\n",
    "\n",
    "The whole process is basically a try-and-error process, since most of your new ideas are heuristic and requires furthur validation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**\n",
    "\n",
    "## 1. Preprocessing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Extract post content from raw data\n",
    "def get_content(entry):\n",
    "    post = entry[\"root\"][\"_source\"][\"post\"]\n",
    "    return {\"id\": post[\"post_id\"], \"text\": post[\"text\"]} # Don't need hashtags as they are included in text\n",
    "\n",
    "with open(\"./data/competition/final_posts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame([get_content(item) for item in raw_data])\n",
    "\n",
    "# Creaete mapping from post_id to split type and emotion\n",
    "df_split = pd.read_csv(\"./data/competition/data_identification.csv\")\n",
    "df_emotion = pd.read_csv(\"./data/competition/emotion.csv\")\n",
    "\n",
    "split_map = dict(zip(df_split['id'], df_split['split']))\n",
    "emotion_map = dict(zip(df_emotion['id'], df_emotion['emotion']))\n",
    "\n",
    "# Mapping and splitting\n",
    "df[\"split\"] = df[\"id\"].map(split_map)\n",
    "df[\"emotion\"] = df[\"id\"].map(emotion_map)\n",
    "df[\"text\"] = df[\"text\"].str.replace(\"#\", \"\", regex=False) # Remove hashtags from text as it might affect model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>split</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x61fc95</td>\n",
       "      <td>We got the ranch, loaded our guns and sat up t...</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x35663e</td>\n",
       "      <td>I bet there is an army of married couples who ...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0xc78afe</td>\n",
       "      <td>This could only end badly.</td>\n",
       "      <td>train</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x90089c</td>\n",
       "      <td>My sister squeezed a lime in her milk when she...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xaba820</td>\n",
       "      <td>and that got my head bobbing a little bit.</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64166</th>\n",
       "      <td>0x4afbe1</td>\n",
       "      <td>Guilty Gear actually did that before with Guil...</td>\n",
       "      <td>train</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64167</th>\n",
       "      <td>0xf5ba78</td>\n",
       "      <td>One of my favorite episodes.</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64168</th>\n",
       "      <td>0x8f758e</td>\n",
       "      <td>I got my first raspberry from a crowd surfer f...</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64169</th>\n",
       "      <td>0xb5a35a</td>\n",
       "      <td>Texans and Astros both shut out tonight. Houst...</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64170</th>\n",
       "      <td>0x3a9174</td>\n",
       "      <td>Pre-prepare direction plays hale and hearty si...</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64171 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text  split  \\\n",
       "0      0x61fc95  We got the ranch, loaded our guns and sat up t...   test   \n",
       "1      0x35663e  I bet there is an army of married couples who ...  train   \n",
       "2      0xc78afe                         This could only end badly.  train   \n",
       "3      0x90089c  My sister squeezed a lime in her milk when she...  train   \n",
       "4      0xaba820         and that got my head bobbing a little bit.   test   \n",
       "...         ...                                                ...    ...   \n",
       "64166  0x4afbe1  Guilty Gear actually did that before with Guil...  train   \n",
       "64167  0xf5ba78                       One of my favorite episodes.  train   \n",
       "64168  0x8f758e  I got my first raspberry from a crowd surfer f...   test   \n",
       "64169  0xb5a35a  Texans and Astros both shut out tonight. Houst...  train   \n",
       "64170  0x3a9174  Pre-prepare direction plays hale and hearty si...   test   \n",
       "\n",
       "       emotion  \n",
       "0          NaN  \n",
       "1          joy  \n",
       "2         fear  \n",
       "3          joy  \n",
       "4          NaN  \n",
       "...        ...  \n",
       "64166    anger  \n",
       "64167      joy  \n",
       "64168      NaN  \n",
       "64169  sadness  \n",
       "64170      NaN  \n",
       "\n",
       "[64171 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Checking Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(89)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"].str.split().str.len().max()\n",
    "# Maximum number of words is 89, so setting max_length of token to 128 shoudd be sufficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "text           0\n",
       "split          0\n",
       "emotion    16281\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()\n",
    "# Emotion labels of test set are set to NaN for later prediction, no other missing values found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "joy         23797\n",
       "anger       10694\n",
       "surprise     6281\n",
       "sadness      3926\n",
       "fear         2009\n",
       "disgust      1183\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"emotion\"].value_counts()\n",
    "# The distiribution of emotion labels is somewhat imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "train_df = df[df[\"split\"] == \"train\"].reset_index(drop=True).drop(columns=[\"split\"])\n",
    "test_df  = df[df[\"split\"] == \"test\"].reset_index(drop=True).drop(columns=[\"split\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map emotion labels to ids\n",
    "emotion_to_id = {\n",
    "    \"anger\": 0,\n",
    "    \"disgust\": 1,\n",
    "    \"fear\": 2,\n",
    "    \"sadness\":3 ,\n",
    "    \"surprise\": 4,\n",
    "    \"joy\": 5,\n",
    "}\n",
    "\n",
    "id_to_emotion = {i: e for e, i in emotion_to_id.items()}\n",
    "\n",
    "train_df[\"labels\"] = train_df[\"emotion\"].map(emotion_to_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Dataset & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Dataset\n",
    "def prepare_dataset(df, tokenizer=None):\n",
    "    train_df, valid_df = train_test_split(\n",
    "        df,\n",
    "        test_size=0.1,\n",
    "        stratify=df[\"labels\"],\n",
    "        random_state=44\n",
    "    )\n",
    "\n",
    "    train_ds = Dataset.from_pandas(train_df)\n",
    "    valid_ds = Dataset.from_pandas(valid_df)\n",
    "\n",
    "    train_ds = train_ds.map(lambda x: tokenizer(x[\"text\"], max_length=128, truncation=True), batched=True)\n",
    "    valid_ds = valid_ds.map(lambda x: tokenizer(x[\"text\"], max_length=128, truncation=True), batched=True)\n",
    "\n",
    "    # Remove unused columns\n",
    "    train_ds = train_ds.remove_columns([\"text\", \"emotion\"])\n",
    "    valid_ds = valid_ds.remove_columns([\"text\", \"emotion\"])\n",
    "\n",
    "    train_ds.set_format(\"torch\")\n",
    "    valid_ds.set_format(\"torch\")\n",
    "\n",
    "    return train_ds, valid_ds\n",
    "\n",
    "# Metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1_macro\": f1_score(labels, preds, average=\"macro\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SamLowe/roberta-base-go_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519e20d362f842a093507cac6f74e949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/43101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8963d46d66de40b4a71c4b8e399c9666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4789 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at SamLowe/roberta-base-go_emotions and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([28]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([28, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8082' max='8082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8082/8082 03:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.992400</td>\n",
       "      <td>0.967241</td>\n",
       "      <td>0.663186</td>\n",
       "      <td>0.465899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.854600</td>\n",
       "      <td>0.884538</td>\n",
       "      <td>0.692003</td>\n",
       "      <td>0.527373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.693000</td>\n",
       "      <td>0.904665</td>\n",
       "      <td>0.690123</td>\n",
       "      <td>0.544889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('[SamLowe]roberta-base-go_emotions/tokenizer_config.json',\n",
       " '[SamLowe]roberta-base-go_emotions/special_tokens_map.json',\n",
       " '[SamLowe]roberta-base-go_emotions/vocab.json',\n",
       " '[SamLowe]roberta-base-go_emotions/merges.txt',\n",
       " '[SamLowe]roberta-base-go_emotions/added_tokens.json',\n",
       " '[SamLowe]roberta-base-go_emotions/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "model_name = \"SamLowe/roberta-base-go_emotions\"\n",
    "model_dir = \"[SamLowe]roberta-base-go_emotions\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "train_ds, valid_ds = prepare_dataset(train_df, tokenizer=tokenizer)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=6,\n",
    "    problem_type=\"single_label_classification\",\n",
    "    id2label=id_to_emotion,\n",
    "    label2id=emotion_to_id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=valid_ds,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(model_dir)\n",
    "tokenizer.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/owenhsieh/miniconda3/envs/dm2025/lib/python3.12/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'anger', 1: 'disgust', 2: 'fear', 3: 'sadness', 4: 'surprise', 5: 'joy'}\n",
      "submission.csv created!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "model_dir = \"[SamLowe]roberta-base-go_emotions\"\n",
    "\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model_dir,\n",
    "    tokenizer=model_dir,\n",
    "    return_all_scores=False,\n",
    "    batch_size=32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(classifier.model.config.id2label)\n",
    "\n",
    "# inference\n",
    "texts = test_df[\"text\"].tolist()\n",
    "outputs = classifier(texts)\n",
    "preds = [o[\"label\"] for o in outputs]\n",
    "pred_df = pd.DataFrame({\"id\": test_df[\"id\"], \"emotion\": preds})\n",
    "\n",
    "# use format of sample submission\n",
    "sample_df = pd.read_csv(\"data/competition/samplesubmission.csv\")\n",
    "final_df = sample_df[[\"id\"]].merge(pred_df, on=\"id\", how=\"left\")\n",
    "\n",
    "# output\n",
    "final_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"submission.csv created!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
